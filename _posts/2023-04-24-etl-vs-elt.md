---
layout: post
title: "ETL vs ELT"
categories: "big-data-fundamentals"
---

In the world of batch big data processing, we often have to answer the typical use-case of extracting data from a source, and loading it into a target.
However the data will probably need to be cleaned or filtered, or different sources need to be combined, so this data has to go through a transformation step.  

And for this there are two strategies:
- Extract, Transform and Load
- Extract, Load and then Transform

   
  
## ETL:
Your data processing happens before loading the result into the target system (a database or a data warehouse). This is a typical use-case for big data processing engines such as [Spark](https://en.wikipedia.org/wiki/Apache_Spark), [Hadoop MapReduce](https://en.wikipedia.org/wiki/Apache_Hadoop) (outdated), or even [Beam](https://en.wikipedia.org/wiki/Apache_Beam) and [Flink](https://en.wikipedia.org/wiki/Apache_Flink). ETL can be done in batch or stream.  

These have high versatility and all types of processing can be covered: structured, unstructured, images, machine learning, graphs etc…    

The drawback is that they require software engineering skills, and knowledge on the technological stack of the platform.  

   
  
## ELT:
The trick with ELT is that almost all transformations can be written as typical SQL queries.
No need for a dedicated processing engine, the target can run the transformations itself, if it is powerful enough.
This is a typical use-case for data warehouse platforms such as [Snowflake](https://en.wikipedia.org/wiki/Snowflake_Inc), [Redshift](https://en.wikipedia.org/wiki/Amazon_Redshift), [BigQuery](https://en.wikipedia.org/wiki/BigQuery).
They are often used with tools to organize the SQL code in processing steps, such as [DBT](https://www.getdbt.com/) or [SQLMesh](https://sqlmesh.com/).  

As they are based on SQL, almost anyone with an analyst background can hop on the platform and process data at scale.  

The drawbacks are that the transformations are limited to semi-structured/structured data, and usually happen in a batch fasion¹. And an important note has to be made about SQL abstracting away the complexities of data processing at scale: it is easy to fall in the trap of recomputing everything with every run.
To avoid this, DBT gives the concept of [incremental models](https://docs.getdbt.com/docs/build/incremental-models#when-should-i-use-an-incremental-model).  

Edit from 2023-12-16:  
¹ This can be challenged by the coming of technologies such as [RisingWave](https://github.com/risingwavelabs/risingwave) or [Materialize](https://github.com/MaterializeInc/materialize). Both bring the possibility of doing stream processing with simple SQL.
With such technologies we could see the challenge of incrementalisation in ELT just plainly disappear.  


  
 
 
## Summary:

- ETL: Covers a lot of ground but requires specialists to use the platform
- ELT: Limited to SQL transformations, very accessible but costly if queries are not optimized.